{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Based Policy Optimization: MBPO in Pendulum\n",
    "\n",
    "基于模型策略优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.9844762 , -0.17551816,  0.25879556], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# 定义环境\n",
    "class MyWrapper(gym.Wrapper):\n",
    "  def __init__(self):\n",
    "    env = gym.make('Pendulum-v1', render_mode='rgb_array')\n",
    "    super().__init__(env)\n",
    "    self.env = env\n",
    "    self.step_n = 0\n",
    "  \n",
    "  def reset(self):\n",
    "    state, _ = self.env.reset()\n",
    "    self.step_n = 0\n",
    "    return state\n",
    "    \n",
    "  def step(self, action):\n",
    "    state, reward, terminated, truncated, info = self.env.step(action)\n",
    "    done = terminated or truncated\n",
    "    self.step_n += 1\n",
    "    if self.step_n >= 200:\n",
    "      done = True\n",
    "    return state, reward, done, info \n",
    "  \n",
    "env = MyWrapper()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.678047239780426"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from IPython import display\n",
    "import math\n",
    "\n",
    "# 基底模型使用SAC\n",
    "class SAC:\n",
    "\tclass ModelAction(torch.nn.Module):\n",
    "\t\tdef __init__(self):\n",
    "\t\t\tsuper().__init__()\n",
    "\t\t\t# 定义模型\n",
    "\t\t\tself.fc_state = torch.nn.Sequential(\n",
    "\t\t\t\ttorch.nn.Linear(3, 128),\n",
    "\t\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\tself.fc_mu = torch.nn.Linear(128, 1)\n",
    "\t\t\tself.fc_std = torch.nn.Sequential(\n",
    "\t\t\t\ttorch.nn.Linear(128, 1),\n",
    "\t\t\t\ttorch.nn.Softplus(),\n",
    "\t\t\t)\n",
    "\n",
    "\t\tdef forward(self, state):\n",
    "\t\t\t# [b, 3] -> [b, 128]\n",
    "\t\t\tstate = self.fc_state(state)\n",
    "\n",
    "\t\t\t# [b, 128] -> [b, 1]\n",
    "\t\t\tmu = self.fc_mu(state)\n",
    "\n",
    "\t\t\t# [b, 128] -> [b, 1]\n",
    "\t\t\tstd = self.fc_std(state)\n",
    "\n",
    "\t\t\t# 定义b个正态分布\n",
    "\t\t\tdist = torch.distributions.Normal(mu, std)\n",
    "\n",
    "\t\t\t# 采样b个样本\n",
    "      # 这里用的是rsample,表示重采样,\n",
    "      # 其实就是先从一个标准正态分布中采样,然后乘以标准差,加上均值\n",
    "\t\t\tsample = dist.rsample()\n",
    "\n",
    "\t\t\t# 样本压缩到-1， 1，求动作\n",
    "\t\t\taction = torch.tanh(sample)\n",
    "\n",
    "\t\t\t# 求概率对数\n",
    "\t\t\tlog_prob = dist.log_prob(sample)\n",
    "  \n",
    "\t\t\t# 动作熵\n",
    "\t\t\tentropy = log_prob - (1 - action.tanh() ** 2 + 1e-7).log()\n",
    "\t\t\tentropy = -entropy\n",
    "\n",
    "\t\t\treturn action * 2, entropy\n",
    "\n",
    "\tclass ModelValue(torch.nn.Module):\n",
    "\t\t\n",
    "\t\tdef __init__(self):\n",
    "\t\t\tsuper().__init__()\n",
    "\t\t\tself.sequential = torch.nn.Sequential(\n",
    "        torch.nn.Linear(4, 128),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 128),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 1),\n",
    "\t\t)\n",
    "    \n",
    "\t\tdef forward(self, state, action):\n",
    "\t\t\t# [b, 3 + 1] -> [b, 4]\n",
    "\t\t\tstate = torch.cat([state, action], dim=1)\n",
    "\n",
    "\t\t\t# [b, 4] -> [b, 1]\n",
    "\t\t\treturn self.sequential(state)\n",
    "  \n",
    "\tdef __init__(self):\n",
    "\t\tself.model_action = self.ModelAction()\n",
    "  \n",
    "\t\tself.model_value1 = self.ModelValue()\n",
    "\t\tself.model_value2 = self.ModelValue()\n",
    "\n",
    "\t\tself.model_value_next1 = self.ModelValue()\n",
    "\t\tself.model_value_next2 = self.ModelValue()\n",
    "\n",
    "\t\tself.model_value_next1.load_state_dict(self.model_value1.state_dict())\n",
    "\t\tself.model_value_next2.load_state_dict(self.model_value2.state_dict())\n",
    "\n",
    "\t\t# alpha 可学习参数\n",
    "\t\tself.alpha = torch.tensor(math.log(0.01))\n",
    "\t\tself.alpha.requires_grad = True\n",
    "  \n",
    "\t\tself.optimizer_action = torch.optim.Adam(self.model_action.parameters(), lr=3e-4)\n",
    "\t\tself.optimizer_value1 = torch.optim.Adam(self.model_value1.parameters(), lr=3e-3)\n",
    "\t\tself.optimizer_value2 = torch.optim.Adam(self.model_value2.parameters(), lr=3e-3)\n",
    "\t\tself.optimizer_alpha = torch.optim.Adam([self.alpha], lr=3e-4)\n",
    "\n",
    "\t\tself.loss_fn = torch.nn.MSELoss()\n",
    "  \n",
    "\tdef get_action(self, state):\n",
    "\t\tstate = torch.FloatTensor(state).reshape(1, 3)\n",
    "\t\taction, _ = self.model_action(state)\n",
    "\t\treturn action.item()\n",
    "\n",
    "\tdef _soft_update(self, model, model_next):\n",
    "\t\tfor param, param_next in zip(model.parameters(), model_next.parameters()):\n",
    "\t\t\t# 小比例更新\n",
    "\t\t\tvalue = param_next.data * 0.995 + param.data * 0.005\n",
    "\t\t\tparam_next.data.copy_(value)\n",
    "\n",
    "\tdef _get_target(self, reward, next_state, over):\n",
    "\t\t# 首先使用model_action计算动作和动作的熵\n",
    "\t\t# [b, 3] -> [b, 1], [b, 1]\n",
    "\t\taction, entropy = self.model_action(next_state)\n",
    "\n",
    "\n",
    "\t\t# 评估next_state的价值\n",
    "\t\t# [b, 4], [b, 1] -> [b, 1]\n",
    "\t\ttarget1 = self.model_value_next1(next_state, action)\n",
    "\t\ttarget2 = self.model_value_next2(next_state, action)\n",
    "\n",
    "\t\t# 取价值小的，出于稳定性\n",
    "\t\t# [b, 1]\n",
    "\t\ttarget = torch.min(target1, target2)\n",
    "\n",
    "\t\t# 还原alpha\n",
    "\t\t# target 加上动作熵，alpha作为权重系数\n",
    "\t\t# [b, 1] - [b, 1] -> [b, 1]\n",
    "\t\ttarget += self.alpha.exp() * entropy\n",
    "\n",
    "\t\t# [b, 1] \n",
    "\t\ttarget *= 0.99\n",
    "\t\ttarget *= (1 - over)\n",
    "\t\ttarget += reward\n",
    "\n",
    "\t\treturn target\n",
    "\n",
    "\tdef _get_loss_action(self, state):\n",
    "\t\t# 计算action和熵\n",
    "\t\t# [b, 3] -> [b, 1], [b, 1]\n",
    "\t\taction, entropy = self.model_action(state)\n",
    "\n",
    "\t\t# 使用两个value网络评估action的价值\n",
    "\t\t#[b, 3],[b, 1] -> [b, 1]\n",
    "\t\tvalue1 = self.model_value1(state, action)\n",
    "\t\tvalue2 = self.model_value2(state, action)\n",
    "\n",
    "\t\t# 取价值小的,出于稳定性考虑\n",
    "\t\t# [b, 1]\n",
    "\t\tvalue = torch.min(value1, value2)\n",
    "\n",
    "\t\t# alpha还原后乘以熵,这个值期望的是越大越好,但是这里是计算loss,所以符号取反\n",
    "\t\t# [1] - [b, 1] -> [b, 1]\n",
    "\t\tloss_action = -self.alpha.exp() * entropy\n",
    "\t\t\n",
    "\t\t# 减去value,所以value越大越好,这样loss就会越小\n",
    "\t\tloss_action -= value\n",
    "\n",
    "\t\treturn loss_action.mean(), entropy\n",
    "\n",
    "\tdef _get_loss_value(self, model_value, target, state, action, next_state):\n",
    "\t\t# 计算value\n",
    "\t\tvalue = model_value(state, action)\n",
    "  \n",
    "\t\t# 计算loss, value, 贴近target\n",
    "\t\tloss_value = self.loss_fn(value, target)\n",
    "\t\treturn loss_value\n",
    "\n",
    "\tdef train(self, state, reward, action, next_state, over):\n",
    "\t\t# 对reward 偏移，便于训练\n",
    "\t\treward = (reward + 8) / 8\n",
    "\n",
    "\t\t# 计算value和target, target已经考虑了动作和熵\n",
    "\t\t# [b, 1]\n",
    "\t\ttarget = self._get_target(reward, next_state, over)\n",
    "\t\ttarget = target.detach()\n",
    "\t\t\n",
    "\t\t# 计算两个value\n",
    "\t\tloss_value1 = self._get_loss_value(self.model_value1, target, state, action, next_state)\n",
    "\t\tloss_value2 = self._get_loss_value(self.model_value2, target, state, action, next_state)\n",
    "\t\t\n",
    "\t\t# 更新参数\n",
    "\t\tself.optimizer_value1.zero_grad()\n",
    "\t\tloss_value1.backward()\n",
    "\t\tself.optimizer_value1.step()\n",
    "\t\t\n",
    "\t\tself.optimizer_value2.zero_grad()\n",
    "\t\tloss_value2.backward()\n",
    "\t\tself.optimizer_value2.step()\n",
    "\t\t\n",
    "\t\t# 使用model_value计算model_action的loss, 更新参数\n",
    "\t\tloss_action, entropy = self._get_loss_action(state)\n",
    "\t\tself.optimizer_action.zero_grad()\n",
    "\t\tloss_action.backward()\n",
    "\t\tself.optimizer_action.step()\n",
    "\t\t\n",
    "\t\t# 熵乘以alpha就是alpha的loss\n",
    "\t\t# [b, 1] -> [1]\n",
    "\t\tloss_alpha = (entropy + 1).detach() * self.alpha.exp()\n",
    "\t\tloss_alpha = loss_alpha.mean()\n",
    "\t\t\n",
    "\t\t# 更新alpha值\n",
    "\t\tself.optimizer_alpha.zero_grad()\n",
    "\t\tloss_alpha.backward()\n",
    "\t\tself.optimizer_alpha.step()\n",
    "\t\t\n",
    "\t\t# 小比例更新\n",
    "\t\tself._soft_update(self.model_value1, self.model_value_next1)\n",
    "\t\tself._soft_update(self.model_value2, self.model_value_next2)\n",
    "  \n",
    "sac = SAC()\n",
    "\n",
    "sac.train(\n",
    "\ttorch.randn(5, 3),\n",
    "\ttorch.randn(5, 1),\n",
    "\ttorch.randn(5, 1),\n",
    "\ttorch.randn(5, 3),\n",
    "\ttorch.zeros(5, 1).long(),\n",
    ")\n",
    "\n",
    "sac.get_action([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,\n",
       " ([0.48654261231422424, 0.8736568689346313, 0.72088223695755],\n",
       "  -1.7668370008468628,\n",
       "  -1.1843527750986558,\n",
       "  [0.4372809827327728, 0.8993249535560608, 1.1110992431640625],\n",
       "  False),\n",
       " (tensor([[-0.1987,  0.9801, -7.1283],\n",
       "          [ 0.9632,  0.2687, -2.3261]]),\n",
       "  tensor([[-1.0721],\n",
       "          [ 1.0418]]),\n",
       "  tensor([[-8.2182],\n",
       "          [-0.6162]]),\n",
       "  tensor([[ 0.1273,  0.9919, -6.5541],\n",
       "          [ 0.9850,  0.1728, -1.9683]]),\n",
       "  tensor([[0],\n",
       "          [0]])))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Pool:\n",
    "  def __init__(self, limit):\n",
    "    # 样本池\n",
    "    self.datas = []\n",
    "    self.limit = limit\n",
    "  \n",
    "\t# 向样本池中添加数据\n",
    "  def add(self, state, action, reward, next_state, over):\n",
    "    if isinstance(state, np.ndarray) or isinstance(state, torch.Tensor):\n",
    "      state = state.reshape(3).tolist()\n",
    "\n",
    "    action = float(action)\n",
    "    reward = float(reward)\n",
    "    \n",
    "    if isinstance(next_state, np.ndarray) or isinstance(next_state, torch.Tensor):\n",
    "      next_state = next_state.reshape(3).tolist()\n",
    "\n",
    "    over = bool(over)\n",
    "\n",
    "    self.datas.append((state, action, reward, next_state, over))\n",
    "\n",
    "    # 数据上限，超出时从最古老的开始删除\n",
    "    while len(self.datas) > self.limit:\n",
    "      self.datas.pop(0)\n",
    "\n",
    "  # 获取一批数据样本\n",
    "  def get_sample(self, size=None):\n",
    "    if size is None:\n",
    "      size = len(self)\n",
    "      \n",
    "    size = min(size, len(self))\n",
    "    \n",
    "    # 从样本池中采样\n",
    "    samples = random.sample(self.datas, size)\n",
    "\n",
    "    # [b, 3]\n",
    "    state = torch.FloatTensor([i[0] for i in samples]).reshape(-1, 3)\n",
    "    # [b, 1]\n",
    "    action = torch.FloatTensor([i[1] for i in samples]).reshape(-1, 1)\n",
    "    # [b, 1]\n",
    "    reward = torch.FloatTensor([i[2] for i in samples]).reshape(-1, 1)\n",
    "    # [b, 4]\n",
    "    next_state = torch.FloatTensor([i[3] for i in samples]).reshape(-1, 3)\n",
    "    # [b, 1]\n",
    "    over = torch.LongTensor([i[4] for i in samples]).reshape(-1, 1)\n",
    "\n",
    "    return state, action, reward, next_state, over\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.datas)\n",
    "\n",
    "env_pool = Pool(100000)\n",
    "model_pool = Pool(1000)\n",
    "\n",
    "# 初始化一局游戏的数据\n",
    "def _():\n",
    "\t# 初始化游戏\n",
    "\tstate = env.reset()\n",
    "\n",
    "\t# 玩到游戏结束为止\n",
    "\tover = False\n",
    "\twhile not over:\n",
    "\t\t# 根据当前状态得到一个动作\n",
    "\t\taction = sac.get_action(state)\n",
    "  \n",
    "\t\t# 执行动作得到反馈\n",
    "\t\tnext_state, reward, over, _ = env.step([action])\n",
    "  \n",
    "\t\t# 记录数据样本\n",
    "\t\tenv_pool.add(state, action, reward, next_state, over)\n",
    "  \n",
    "\t\t# 更新游戏状态，开始下一个动作\n",
    "\t\tstate = next_state\n",
    "  \n",
    "\n",
    "_()\n",
    "\n",
    "len(env_pool), env_pool.datas[0], env_pool.get_sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 64, 4]), torch.Size([5, 64, 4]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义主模型\n",
    "class Model(torch.nn.Module):\n",
    "  \n",
    "  # swish 激活函数\n",
    "  class Swish(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "      return x * torch.sigmoid(x)\n",
    "    \n",
    "  # 定义工具层\n",
    "  class FCLayer(torch.nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "      super().__init__()\n",
    "      self.in_size = in_size\n",
    "      \n",
    "      # 初始化参数\n",
    "      std = in_size ** 0.5\n",
    "      std *= 2\n",
    "      std = 1 / std\n",
    "      \n",
    "      weight = torch.empty(5, in_size, out_size)\n",
    "      torch.nn.init.normal_(weight, mean=0.0, std=std)\n",
    "\n",
    "      \n",
    "      # [5, in, out]\n",
    "      self.weight = torch.nn.Parameter(weight)\n",
    "      \n",
    "      # [5, 1, out]\n",
    "      self.bias = torch.nn.Parameter(torch.zeros(5, 1, out_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "      # x -> [5, b, in]\n",
    "      \n",
    "      # [5, b, in] * [5, in, out] -> [5, b, out]\n",
    "      x = torch.bmm(x, self.weight)\n",
    "      \n",
    "      # [5, b, out] + [5, 1, out] -> [5, b, out]\n",
    "      x = x + self.bias\n",
    "      \n",
    "      return x\n",
    "    \n",
    "    \n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.sequential = torch.nn.Sequential(\n",
    "      self.FCLayer(4, 200),\n",
    "      self.Swish(),\n",
    "      self.FCLayer(200, 200),\n",
    "      self.Swish(),\n",
    "      self.FCLayer(200, 200),\n",
    "      self.Swish(),\n",
    "      self.FCLayer(200, 200),\n",
    "      self.Swish(),\n",
    "      self.FCLayer(200, 8),\n",
    "      torch.nn.Identity(),\n",
    "    )\n",
    "    \n",
    "    self.softplus = torch.nn.Softplus()\n",
    "    self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        \n",
    "  def forward(self, x):\n",
    "    # x -> [5, b, 4]\n",
    "    \n",
    "    # [5, b, 4] -> [5, b, 8]\n",
    "    x = self.sequential(x)\n",
    "    \n",
    "    # [5, b, 8] -> [5, b, 4]\n",
    "    mean = x[..., :4]\n",
    "    \n",
    "    # [5, b, 8] -> [5, b, 4]\n",
    "    logvar = x[..., 4:]\n",
    "    \n",
    "    # [1, 1, 4] - [5, b, 4] -> [5, b, 4]\n",
    "    logvar = 0.5 - logvar\n",
    "    \n",
    "    # [1, 1, 4] - [5, b, 4] -> [5, b, 4]\n",
    "    logvar = 0.5 - self.softplus(logvar)\n",
    "    \n",
    "    # [5, b, 4] - [1, 1, 4] -> [5, b, 4]\n",
    "    logvar = logvar + 10\n",
    "    \n",
    "    # [5, b, 4] + [1, 1, 4] -> [5, b, 4]\n",
    "    logvar = self.softplus(logvar) - 10\n",
    "    \n",
    "    #[5, b, 4],[5, b, 4]\n",
    "    return mean, logvar\n",
    "  \n",
    "  def train(self):\n",
    "    state, action, reward, next_state, _ = env_pool.get_sample()\n",
    "    # input -> [b, 4]\n",
    "    # label -> [b, 4]\n",
    "    input = torch.cat([state, action], dim=1)\n",
    "    label = torch.cat([reward, next_state - state], dim=1)\n",
    "    \n",
    "    # 反复训练N次\n",
    "    for _ in range(len(input) // 64 * 20):\n",
    "      #从全量数据中抽样64个,反复抽5遍,形成5份数据\n",
    "      #[5, 64]\n",
    "      select = [torch.randperm(len(input))[:64] for _ in range(5)]\n",
    "      select = torch.stack(select)\n",
    "      # [5, b, 4], [5, b, 4]\n",
    "      input_select = input[select]\n",
    "      label_select = label[select]\n",
    "      del select\n",
    "      \n",
    "      # 模型计算\n",
    "      # [5, b, 4] -> [5, b, 4], [5, b, 4]\n",
    "      mean, logvar = model(input_select)\n",
    "      \n",
    "      # 计算loss\n",
    "      # [b, 4] - [b, 4] * [b, 4] -> [b, 4]\n",
    "      mse_loss = (mean - label_select) ** 2 * (-logvar).exp()\n",
    "      \n",
    "      # [b, 4] -> [b] -> scala\n",
    "      mse_loss = mse_loss.mean(dim=1).mean()\n",
    "      \n",
    "      # [b, 4] -> [b] -> scala\n",
    "      var_loss = logvar.mean(dim=1).mean()\n",
    "      \n",
    "      loss = mse_loss + var_loss\n",
    "      \n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "      \n",
    "model = Model()\n",
    "a, b = model(torch.randn(5, 64, 4))\n",
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1]) torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "class MBPO():\n",
    "  def _fake_step(self, state, action):\n",
    "    state = torch.FloatTensor(state).reshape(-1, 3)\n",
    "    action = torch.FloatTensor([action]).reshape(-1, 1)\n",
    "    # state -> [b, 3]\n",
    "    # action -> [b, 1]\n",
    "    \n",
    "    # [b, 4]\n",
    "    input = torch.cat([state, action], dim=1)\n",
    "    \n",
    "    # 重复5遍\n",
    "    # [b, 4] -> [1, b, 4] -> [5, b, 4]\n",
    "    input = input.unsqueeze(dim=0).repeat([5, 1, 1])\n",
    "    \n",
    "    # 模型计算\n",
    "    # [5, b, 4] -> [5, b, 4],[5, b, 4]\n",
    "    with torch.no_grad():\n",
    "      mean, std = model(input)\n",
    "    std = std.exp().sqrt()\n",
    "    del input\n",
    "    \n",
    "    # means的后三列加上环境数据\n",
    "    mean[:, :, 1:] += state\n",
    "    \n",
    "    # 重采样\n",
    "    # [5, b, 4]\n",
    "    sample = torch.distributions.Normal(0, 1).sample(mean.shape)\n",
    "    sample = mean + sample * std\n",
    "    \n",
    "    # 0-4采样b个元素\n",
    "    # [4, 4, 2, 4, 3, 4, 1, 3, 3, 0, 2, ......]\n",
    "    select = [random.choice(range(5)) for _ in range(mean.shape[1])]\n",
    "    \n",
    "    # 重采样结果，0d: 0-4随机选择，2d: 0-b随机选择\n",
    "    # [5, b, 4] -> [b, 4]\n",
    "    sample = sample[select, range(mean.shape[1])]\n",
    "    \n",
    "    # 切分\n",
    "    reward, next_state = sample[:, :1], sample[:, 1:]\n",
    "\n",
    "    return reward, next_state\n",
    "    \n",
    "  def rollout(self):\n",
    "    states, _, _, _, _ = env_pool.get_sample(1000)\n",
    "    for state in states:\n",
    "      action = sac.get_action(state)\n",
    "      reward, next_state = self._fake_step(state, action)\n",
    "      \n",
    "      model_pool.add(state, action, reward, next_state, False)\n",
    "      state = next_state\n",
    "  \n",
    "  \n",
    "mbpo = MBPO()\n",
    "a, b, = mbpo._fake_step([1, 2, 3], 1)\n",
    "print(a.shape, b.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4400 1000 -1501.6687599952068\n",
      "1 4600 1000 -1648.8052691166113\n",
      "2 4800 1000 -1658.6450614823418\n",
      "3 5000 1000 -1423.7590339890028\n",
      "4 5200 1000 -1601.9574772193978\n",
      "5 5400 1000 -1497.145118584554\n",
      "6 5600 1000 -1549.1217911468475\n",
      "7 5800 1000 -1659.984712824094\n",
      "8 6000 1000 -1493.9186326216739\n",
      "9 6200 1000 -1664.9796748770852\n",
      "10 6400 1000 -1439.3199169855939\n",
      "11 6600 1000 -1494.0383525766676\n",
      "12 6800 1000 -1680.307633952045\n",
      "13 7000 1000 -1363.6668641944696\n",
      "14 7200 1000 -1556.745940425559\n",
      "15 7400 1000 -1634.7677005951002\n",
      "16 7600 1000 -1622.1893034855896\n",
      "17 7800 1000 -1598.9566343098154\n",
      "18 8000 1000 -1624.661010674327\n",
      "19 8200 1000 -1612.9941092380554\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "  reward_sum = 0\n",
    "  state = env.reset()\n",
    "  over = False\n",
    "  \n",
    "  step = 0\n",
    "  while not over:\n",
    "    # 每隔50个step, 训练一次模型\n",
    "    if step % 50 == 0:\n",
    "      model.train()\n",
    "      mbpo.rollout()\n",
    "      \n",
    "    step += 1\n",
    "  \n",
    "    # 使用sac获取一个动作\n",
    "    action = sac.get_action(state)\n",
    "\n",
    "    # 执行动作得到反馈\n",
    "    next_state, reward, over, _ = env.step([action])\n",
    "\n",
    "    # 累加reward\n",
    "    reward_sum += reward\n",
    "    \n",
    "    # 记录数据样本\n",
    "    env_pool.add(state, action, reward, next_state, over)\n",
    "\n",
    "    # 更新游戏状态，开始下一个动作\n",
    "    state = next_state\n",
    "    \n",
    "    # 更新模型\n",
    "    for _ in range(10):\n",
    "      sample = []\n",
    "      sample_env = env_pool.get_sample(32)\n",
    "      sample_model = model_pool.get_sample(32)\n",
    "      \n",
    "      for (i1, i2) in zip(sample_env, sample_model):\n",
    "        i3 = torch.cat([i1, i2], dim=0)\n",
    "        sample.append(i3)\n",
    "        \n",
    "      sac.train(*sample)\n",
    "      \n",
    "  \n",
    "  print(i, len(env_pool), len(model_pool), reward_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_learning2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
