{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO: discrete action\n",
    "\n",
    "proximal policy optimization: 近端策略优化\n",
    "\n",
    "状态函数： $V(s) = \\text{所有动作求和} → \\text{概率}(a) * Q(s, a)$\n",
    "\n",
    "修改为：\n",
    "$$\n",
    "V(s) = \\text{所有动作求和} → \\text{现概率}(a) * [\\text{旧概率}(a) / \\text{新概率}(a)] * Q(s, a)\n",
    "$$\n",
    "\n",
    "初始现概率和旧概率相等，但随着模型更新现概率会变化\n",
    "\n",
    "$Q(s, a)$ 用蒙特卡洛估计\n",
    "\n",
    "按照策略梯度的理论,状态价值取决于动作的质量,所以只要最大化V函数,就可以得到最好的动作策略."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# 定义环境\n",
    "class MyWrapper(gym.Wrapper):\n",
    "  \n",
    "  def __init__(self):\n",
    "    env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "    super().__init__(env)\n",
    "    self.env = env\n",
    "    self.step_n = 0\n",
    "    \n",
    "  def reset(self):\n",
    "    state, _ = self.env.reset()\n",
    "    self.step_n = 0\n",
    "    return state\n",
    "  \n",
    "  def step(self, action):\n",
    "    state, reward, terminated, truncated, info = self.env.step(action)\n",
    "    over = terminated or truncated\n",
    "\n",
    "    #限制最大步数\n",
    "    self.step_n += 1\n",
    "    if self.step_n >= 200:\n",
    "        over = True\n",
    "    \n",
    "    #没坚持到最后,扣分\n",
    "    if over and self.step_n < 200:\n",
    "        reward = -1000\n",
    "\n",
    "    return state, reward, over\n",
    "  \n",
    "  # 打印游戏图像\n",
    "  def show(self):\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(self.env.render())\n",
    "    plt.show()\n",
    "    \n",
    "env = MyWrapper()\n",
    "\n",
    "env.reset()\n",
    "\n",
    "env.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4383, 0.5617],\n",
       "         [0.4572, 0.5428]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[-0.1015],\n",
       "         [-0.2263]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model_action = torch.nn.Sequential(\n",
    "\ttorch.nn.Linear(4, 64),\n",
    "\ttorch.nn.ReLU(),\n",
    "\ttorch.nn.Linear(64, 64),\n",
    "\ttorch.nn.ReLU(),\n",
    "\ttorch.nn.Linear(64, 2),\n",
    "\ttorch.nn.Softmax(dim=1),\n",
    ")\n",
    "\n",
    "model_value = torch.nn.Sequential(\n",
    "\ttorch.nn.Linear(4, 64),\n",
    "\ttorch.nn.ReLU(),\n",
    "\ttorch.nn.Linear(64, 64),\n",
    "\ttorch.nn.ReLU(),\n",
    "\ttorch.nn.Linear(64, 1),\n",
    ")\n",
    "\n",
    "model_action(torch.randn(2, 4)), model_value(torch.randn(2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fanyu\\AppData\\Local\\Temp\\ipykernel_20320\\3991560084.py:34: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  state = torch.FloatTensor(state).reshape(-1, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-987.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "import random\n",
    "\n",
    "\n",
    "#玩一局游戏并记录数据\n",
    "def play(show=False):\n",
    "\tstate = []\n",
    "\taction = []\n",
    "\treward = []\n",
    "\tnext_state = []\n",
    "\tover = []\n",
    "\n",
    "\ts = env.reset()\n",
    "\to = False\n",
    "\twhile not o:\n",
    "\t\t# 根据环境采样\n",
    "\t\tprob = model_action(torch.FloatTensor(s).reshape(1, 4))[0].tolist()\n",
    "\t\ta = random.choices(range(2), weights=prob, k=1)[0]\n",
    "\t\t\n",
    "\t\tns, r, o = env.step(a)\n",
    "\n",
    "\t\tstate.append(s)\n",
    "\t\taction.append(a)\n",
    "\t\treward.append(r)\n",
    "\t\tnext_state.append(ns)\n",
    "\t\tover.append(o)\n",
    "  \n",
    "\t\ts = ns\n",
    "\n",
    "\t\tif show:\n",
    "\t\t\tdisplay.clear_output(wait=True)\n",
    "\t\t\tenv.show()\n",
    "  \n",
    "\tstate = torch.FloatTensor(state).reshape(-1, 4)\n",
    "\taction = torch.LongTensor(action).reshape(-1, 1)\n",
    "\treward = torch.FloatTensor(reward).reshape(-1, 1)\n",
    "\tnext_state = torch.FloatTensor(next_state).reshape(-1, 4)\n",
    "\tover = torch.LongTensor(over).reshape(-1, 1)\n",
    "\n",
    "\treturn state, action, reward, next_state, over, reward.sum().item()\n",
    "\n",
    "state, action, reward, next_state, over, reward_sum = play()\n",
    "\n",
    "reward_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_action = torch.optim.Adam(model_action.parameters(), lr=1e-3)\n",
    "optimizer_value = torch.optim.Adam(model_value.parameters(), lr=1e-2)\n",
    "\n",
    "def requires_grad(model, value):\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad_(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# critic 模型训练\n",
    "def train_value(state, reward, next_state, over):\n",
    "  requires_grad(model_action, False)\n",
    "  requires_grad(model_value, True)\n",
    "  \n",
    "  # 计算targets\n",
    "  with torch.no_grad():\n",
    "    target = model_value(next_state)\n",
    "  target = target * 0.98 * (1 - over) + reward\n",
    "  \n",
    "  # 每批数据反复训练10次\n",
    "  for _ in range(10):\n",
    "    # 计算value\n",
    "    value = model_value(state)\n",
    "    \n",
    "    loss = torch.nn.functional.mse_loss(value, target)\n",
    "    loss.backward()\n",
    "    optimizer_value.step()\n",
    "    optimizer_value.zero_grad()\n",
    "  \n",
    "  # 减去value相当于去基线\n",
    "  return (target - value).detach()\n",
    "\n",
    "value = train_value(state, reward, next_state, over)\n",
    "value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295.2062072753906"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练actor\n",
    "def train_action(state, action, value):\n",
    "  requires_grad(model_action, True)\n",
    "  requires_grad(model_value, False)\n",
    "  \n",
    "  # 计算当前state的价值,其实就是Q(state,action),\n",
    "  # 这里是用蒙特卡洛法估计的\n",
    "  delta = []\n",
    "  for i in range(len(value)):\n",
    "    s = 0\n",
    "    for j in range(i, len(value)):\n",
    "      s += value[j] * (0.98 * 0.95) ** (j - i)\n",
    "    delta.append(s)\n",
    "  delta = torch.FloatTensor(delta).reshape(-1, 1)\n",
    "  \n",
    "  # 更新前动作概率\n",
    "  with torch.no_grad():\n",
    "    prob_old = model_action(state).gather(dim=1, index=action)\n",
    "    \n",
    "  # 每批数据反复训练10次\n",
    "  for _ in range(10):\n",
    "    # 更新后的动作概率\n",
    "    prob_new = model_action(state).gather(dim=1, index=action)\n",
    "    # 求出概率的变化\n",
    "    ratio = prob_new / prob_old\n",
    "    \n",
    "    # 计算截断和不截断的两份loss, 取其中小的\n",
    "    surr1 = ratio * delta\n",
    "    surr2 = ratio.clamp(0.8, 1.2) * delta\n",
    "    \n",
    "    loss = -torch.min(surr1, surr2).mean()\n",
    "  \n",
    "    # 更新参数\n",
    "    loss.backward()\n",
    "    optimizer_action.step()\n",
    "    optimizer_action.zero_grad()\n",
    "  \n",
    "  return loss.item()\n",
    "\n",
    "train_action(state, action, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 15.426230430603027 -986.05\n",
      "100 8.471131324768066 200.0\n",
      "200 0.4041866064071655 200.0\n",
      "300 -0.17779764533042908 200.0\n",
      "400 0.4812454283237457 200.0\n",
      "500 -0.3726245164871216 200.0\n",
      "600 0.442292720079422 200.0\n",
      "700 -13.980325698852539 200.0\n",
      "800 1.0609508752822876 200.0\n",
      "900 -1.5280680656433105 200.0\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "def train():\n",
    "\tmodel_action.train()\n",
    "\tmodel_value.train()\n",
    "\n",
    "\t# 训练N局\n",
    "\tfor epoch in range(1000):\n",
    "\t\t# 一个epoch玩N步\n",
    "\t\tsteps = 0\n",
    "\t\twhile steps < 200:\n",
    "\t\t\t# 玩一局，得到数据\n",
    "\t\t\tstate, action, reward, next_state, over, _ = play()\n",
    "\t\t\tsteps += len(state)\n",
    "\n",
    "\t\t\t# 训练两个模型\n",
    "\t\t\tdelta = train_value(state, reward, next_state, over)\n",
    "\t\t\tloss = train_action(state, action, delta)\n",
    "\n",
    "\t\tif epoch % 100 == 0:\n",
    "\t\t\ttest_result = sum([play()[-1] for _ in range(20)]) / 20\n",
    "\t\t\tprint(epoch, loss, test_result)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play(True)[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_learning2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
