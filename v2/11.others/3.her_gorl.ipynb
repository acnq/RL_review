{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others: Hindsight Experience Replay (HER) in Goal Oriented Reinforcement Learning (GoRL)\n",
    "\n",
    "目标导向强化学习： 事后观察经验回放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 定义环境\n",
    "class MyWrapper:\n",
    "  \n",
    "  def reset(self):\n",
    "    # 前两个数是起点,后两个数是终点\n",
    "    self.state = np.random.uniform(size=4, low=-5, high=5)\n",
    "    self.step_n = 0\n",
    "    return self.state.tolist()\n",
    "  \n",
    "  def step(self, action):\n",
    "    action = np.array(action).reshape(2)\n",
    "    \n",
    "    # 裁剪动作范围\n",
    "    action = action.clip(min=-1, max=1)\n",
    "    \n",
    "    # 执行动作\n",
    "    self.state[:2] += action\n",
    "    \n",
    "    # 规范状态空间\n",
    "    self.state[:2] = self.state[:2].clip(min=-5, max=5)\n",
    "    \n",
    "    # 求距离终点的距离\n",
    "    dist = np.linalg.norm(self.state[:2] - self.state[2:], ord=2)\n",
    "    \n",
    "    # 判断到达终点\n",
    "    reward = -1.0\n",
    "    over = False\n",
    "    if dist < 0.1:\n",
    "      reward = 1.0\n",
    "      over = True\n",
    "      \n",
    "    # 限制最大步数\n",
    "    self.step_n += 1\n",
    "    if self.step_n >= 50:\n",
    "      over = True\n",
    "      \n",
    "    return self.state.tolist(), reward, over\n",
    "  \n",
    "  \n",
    "  def show(self):\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.xlim(-5, 5)\n",
    "    plt.ylim(-5, 5)\n",
    "    \n",
    "    plt.plot(*self.state[:2], 'o')\n",
    "    plt.plot(*self.state[2:], 'x')\n",
    "    plt.show()\n",
    "    \n",
    "env = MyWrapper()\n",
    "env.reset()\n",
    "env.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ddpg.DDPG at 0x1cf15506c10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ddpg import DDPG \n",
    "import torch\n",
    "\n",
    "ddpg = DDPG()\n",
    "\n",
    "ddpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-50.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "import random\n",
    "\n",
    "\n",
    "#玩一局游戏并记录数据\n",
    "def play(show=False):\n",
    "  data = []\n",
    "  reward_sum = 0\n",
    "\n",
    "  state = env.reset()\n",
    "  over = False\n",
    "  while not over:\n",
    "    # 根据环境采样\n",
    "    action = ddpg.model_action(torch.FloatTensor(state).reshape(1, 4)).reshape(2)\n",
    "\n",
    "    # 给动作添加噪声，增加探索\n",
    "    action += torch.randn(2) * 0.1\n",
    "    action = action.tolist()\n",
    "    \n",
    "    next_state, reward, over = env.step(action)\n",
    "\n",
    "    data.append((state, action, reward, next_state, over))\n",
    "    reward_sum += reward\n",
    "    \n",
    "    state = next_state\n",
    "\n",
    "    if show:\n",
    "      display.clear_output(wait=True)\n",
    "      env.show()\n",
    "\n",
    "  return data, reward_sum\n",
    "\n",
    "play()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3.818060549821821,\n",
       "  -3.3657802926518587,\n",
       "  0.2761244502925546,\n",
       "  -4.463250486753193],\n",
       " [-0.4177893400192261, -0.38885384798049927],\n",
       " -1.0,\n",
       " [3.400271209802595,\n",
       "  -3.754634140632358,\n",
       "  0.2761244502925546,\n",
       "  -4.463250486753193],\n",
       " False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从一局游戏中取一条伪终点的数据\n",
    "def get_fake_goal_data(data_game, step):\n",
    "  # 取出step的数据\n",
    "  state, action, reward, next_state, over = data_game[step]\n",
    "  \n",
    "  # 随机step后面的某一步数据\n",
    "  step = random.randint(step + 1, len(data_game) - 1)\n",
    "  fake_goal_state = data_game[step][0]\n",
    "  \n",
    "  # 以伪终点构建新的state\n",
    "  state[2:] = fake_goal_state[:2]\n",
    "  next_state[2:] = fake_goal_state[:2]\n",
    "  \n",
    "  # 求距离终点的距离\n",
    "  dist = [next_state[0] - next_state[2], next_state[1] - next_state[3]]\n",
    "  dist = np.linalg.norm(dist, ord=2)\n",
    "  \n",
    "  # 重新计算reward和over\n",
    "  reward = -1.0\n",
    "  over = False\n",
    "  if dist < 0.1:\n",
    "    reward = 1.0\n",
    "    over = True\n",
    "  \n",
    "  # 返回作为伪终点的数据\n",
    "  return state, action, reward, next_state, over\n",
    "\n",
    "get_fake_goal_data(play()[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,\n",
       " ([0.9001722829290788,\n",
       "   -4.863360210997199,\n",
       "   0.11918964133974441,\n",
       "   4.4034108926087665],\n",
       "  [-1.009590744972229, -0.9393027424812317],\n",
       "  -1.0,\n",
       "  [-0.0998277170709212, -5.0, 0.11918964133974441, 4.4034108926087665],\n",
       "  False))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Pool:\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.pool = []\n",
    "    \n",
    "  def __len__(self):\n",
    "    return sum(len(i) for i in self.pool)\n",
    "  \n",
    "  def __getitem__(self, i):\n",
    "    return self.pool[i]\n",
    "  \n",
    "  # 更新动作\n",
    "  def update(self):\n",
    "    # 每次更新至少N条数据\n",
    "    old_len = len(self)\n",
    "    while len(self) - old_len < 200:\n",
    "      self.pool.append(play()[0])\n",
    "      \n",
    "    # 保留最新N条数据\n",
    "    while len(self) > 2_0000:\n",
    "      self.pool = self.pool[1:]\n",
    "      \n",
    "\n",
    "  # 获取一批数据样本\n",
    "  def sample(self):\n",
    "    data = []\n",
    "    for _ in range(64):\n",
    "      # 随机一局游戏\n",
    "      data_game = random.choice(self.pool)\n",
    "      \n",
    "      # 随机取游戏中的一步数据\n",
    "      step = random.randint(0, len(data_game) - 1)\n",
    "      data_step = data_game[step]\n",
    "      \n",
    "      # 随机替换为伪终点数据\n",
    "      if step < len(data_game) - 1 and random.random() < 0.8:\n",
    "        data_step = get_fake_goal_data(data_game, step)\n",
    "        \n",
    "      data.append(data_step)\n",
    "    \n",
    "    state = torch.FloatTensor([i[0] for i in data]).reshape(-1, 4)\n",
    "    action = torch.FloatTensor([i[1] for i in data]).reshape(-1, 2)\n",
    "    reward = torch.FloatTensor([i[2] for i in data]).reshape(-1, 1)\n",
    "    next_state = torch.FloatTensor([i[3] for i in data]).reshape(-1, 4)\n",
    "    over = torch.LongTensor([i[4] for i in data]).reshape(-1, 1)\n",
    "    \n",
    "    return state, action, reward, next_state, over\n",
    "\n",
    "pool = Pool()\n",
    "pool.update()\n",
    "pool.sample()\n",
    "\n",
    "len(pool), pool[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 400 -48.05\n",
      "100 19983 -37.95\n",
      "200 19984 -4.5\n",
      "300 19992 -4.9\n",
      "400 19992 -4.15\n",
      "500 19996 -4.35\n",
      "600 19989 -4.8\n",
      "700 19990 -4.8\n",
      "800 19993 -6.0\n",
      "900 19994 -5.8\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "def train():\n",
    "\t# 共更新N轮数据\n",
    "  for epoch in range(1000):\n",
    "    pool.update()\n",
    "    \n",
    "    # 每次更新过数据后,学习N次\n",
    "    for i in range(200):\n",
    "      # 采样一批\n",
    "      state, action, reward, next_state, over = pool.sample()\n",
    "\n",
    "      # 训练\n",
    "      ddpg.train_action(state)\n",
    "      ddpg.train_value(state, action, reward, next_state, over)\n",
    "      ddpg.soft_update()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "      test_result = sum([play()[-1] for _ in range(20)]) / 20\n",
    "      print(epoch, len(pool), test_result)\n",
    "      \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play(True)[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_learning2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
