{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others: Hindsight Experience Replay (HER) in Goal Oriented Reinforcement Learning (GoRL)\n",
    "\n",
    "目标导向强化学习： 事后观察经验回放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 定义环境\n",
    "class MyWrapper:\n",
    "  \n",
    "  def reset(self):\n",
    "    # 前两个数是起点,后两个数是终点\n",
    "    self.state = np.random.uniform(size=4, low=-5, high=5)\n",
    "    self.step_n = 0\n",
    "    return self.state.tolist()\n",
    "  \n",
    "  def step(self, action):\n",
    "    action = np.array(action).reshape(2)\n",
    "    \n",
    "    # 裁剪动作范围\n",
    "    action = action.clip(min=-1, max=1)\n",
    "    \n",
    "    # 执行动作\n",
    "    self.state[:2] += action\n",
    "    \n",
    "    # 规范状态空间\n",
    "    self.state[:2] = self.state[:2].clip(min=-5, max=5)\n",
    "    \n",
    "    # 求距离终点的距离\n",
    "    dist = np.linalg.norm(self.state[:2] - self.state[2:], ord=2)\n",
    "    \n",
    "    # 判断到达终点\n",
    "    reward = -1.0\n",
    "    over = False\n",
    "    if dist < 0.1:\n",
    "      reward = 1.0\n",
    "      over = True\n",
    "      \n",
    "    # 限制最大步数\n",
    "    self.step_n += 1\n",
    "    if self.step_n >= 50:\n",
    "      over = True\n",
    "      \n",
    "    return self.state.tolist(), reward, over\n",
    "  \n",
    "  \n",
    "  def show(self):\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.xlim(-5, 5)\n",
    "    plt.ylim(-5, 5)\n",
    "    \n",
    "    plt.plot(*self.state[:2], 'o')\n",
    "    plt.plot(*self.state[2:], 'x')\n",
    "    plt.show()\n",
    "    \n",
    "env = MyWrapper()\n",
    "env.reset()\n",
    "env.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ddpg.DDPG at 0x2aef46473d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ddpg import DDPG \n",
    "import torch\n",
    "\n",
    "ddpg = DDPG()\n",
    "\n",
    "ddpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-50.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "import random\n",
    "\n",
    "\n",
    "#玩一局游戏并记录数据\n",
    "def play(show=False):\n",
    "  data = []\n",
    "  reward_sum = 0\n",
    "\n",
    "  state = env.reset()\n",
    "  over = False\n",
    "  while not over:\n",
    "    # 根据环境采样\n",
    "    action = ddpg.model_action(torch.FloatTensor(state).reshape(1, 4)).reshape(2)\n",
    "\n",
    "    # 给动作添加噪声，增加探索\n",
    "    action += torch.randn(2) * 0.1\n",
    "    action = action.tolist()\n",
    "    \n",
    "    next_state, reward, over = env.step(action)\n",
    "\n",
    "    data.append((state, action, reward, next_state, over))\n",
    "    reward_sum += reward\n",
    "    \n",
    "    state = next_state\n",
    "\n",
    "    if show:\n",
    "      display.clear_output(wait=True)\n",
    "      env.show()\n",
    "\n",
    "  return data, reward_sum\n",
    "\n",
    "play()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-2.4308068218595382, -4.657759560990988, 5.0, 3.994603575419725],\n",
       " [0.5086904168128967, 0.11522974818944931],\n",
       " -1.0,\n",
       " [-1.9221164050466415, -4.542529812801539, 5.0, 3.994603575419725],\n",
       " False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从一局游戏中取一条伪终点的数据\n",
    "def get_fake_goal_data(data_game, step):\n",
    "  # 取出step的数据\n",
    "  state, action, reward, next_state, over = data_game[step]\n",
    "  \n",
    "  # 随机step后面的某一步数据\n",
    "  step = random.randint(step + 1, len(data_game) - 1)\n",
    "  fake_goal_state = data_game[step][0]\n",
    "  \n",
    "  # 以伪终点构建新的state\n",
    "  state[2:] = fake_goal_state[:2]\n",
    "  next_state[2:] = fake_goal_state[:2]\n",
    "  \n",
    "  # 求距离终点的距离\n",
    "  dist = [next_state[0] - next_state[2], next_state[1] - next_state[3]]\n",
    "  dist = np.linalg.norm(dist, ord=2)\n",
    "  \n",
    "  # 重新计算reward和over\n",
    "  reward = -1.0\n",
    "  over = False\n",
    "  if dist < 0.1:\n",
    "    reward = 1.0\n",
    "    over = True\n",
    "  \n",
    "  # 返回作为伪终点的数据\n",
    "  return state, action, reward, next_state, over\n",
    "\n",
    "get_fake_goal_data(play()[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,\n",
       " ([-4.537123834744524,\n",
       "   4.439853111305558,\n",
       "   0.6322902699825148,\n",
       "   1.9848531264588098],\n",
       "  [0.024813681840896606, 0.24908727407455444],\n",
       "  -1.0,\n",
       "  [-4.512310152903628,\n",
       "   4.688940385380112,\n",
       "   0.6322902699825148,\n",
       "   1.9848531264588098],\n",
       "  False))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Pool:\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.pool = []\n",
    "    \n",
    "  def __len__(self):\n",
    "    return sum(len(i) for i in self.pool)\n",
    "  \n",
    "  def __getitem__(self, i):\n",
    "    return self.pool[i]\n",
    "  \n",
    "  # 更新动作\n",
    "  def update(self):\n",
    "    # 每次更新至少N条数据\n",
    "    old_len = len(self)\n",
    "    while len(self) - old_len < 200:\n",
    "      self.pool.append(play()[0])\n",
    "      \n",
    "    # 保留最新N条数据\n",
    "    while len(self) > 2_0000:\n",
    "      self.pool = self.pool[1:]\n",
    "      \n",
    "\n",
    "  # 获取一批数据样本\n",
    "  def sample(self):\n",
    "    data = []\n",
    "    for _ in range(64):\n",
    "      # 随机一局游戏\n",
    "      data_game = random.choice(self.pool)\n",
    "      \n",
    "      # 随机取游戏中的一步数据\n",
    "      step = random.randint(0, len(data_game) - 1)\n",
    "      data_step = data_game[step]\n",
    "      \n",
    "      # 随机替换为伪终点数据\n",
    "      if step < len(data_game) - 1 and random.random() < 0.8:\n",
    "        data_step = get_fake_goal_data(data_game, step)\n",
    "        \n",
    "      data.append(data_step)\n",
    "    \n",
    "    state = torch.FloatTensor([i[0] for i in data]).reshape(-1, 4)\n",
    "    action = torch.FloatTensor([i[1] for i in data]).reshape(-1, 2)\n",
    "    reward = torch.FloatTensor([i[2] for i in data]).reshape(-1, 1)\n",
    "    next_state = torch.FloatTensor([i[3] for i in data]).reshape(-1, 4)\n",
    "    over = torch.LongTensor([i[4] for i in data]).reshape(-1, 1)\n",
    "    \n",
    "    return state, action, reward, next_state, over\n",
    "\n",
    "pool = Pool()\n",
    "pool.update()\n",
    "pool.sample()\n",
    "\n",
    "len(pool), pool[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 400 -50.0\n",
      "100 19962 -50.0\n",
      "200 19979 -33.1\n",
      "300 19994 -13.0\n",
      "400 19985 -5.2\n",
      "500 19999 -5.8\n",
      "600 19993 -4.25\n",
      "700 19999 -4.95\n",
      "800 19995 -4.95\n",
      "900 19993 -4.95\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "def train():\n",
    "\t# 共更新N轮数据\n",
    "  for epoch in range(1000):\n",
    "    pool.update()\n",
    "    \n",
    "    # 每次更新过数据后,学习N次\n",
    "    for i in range(200):\n",
    "      # 采样一批\n",
    "      state, action, reward, next_state, over = pool.sample()\n",
    "\n",
    "      # 训练\n",
    "      ddpg.train_action(state)\n",
    "      ddpg.train_value(state, action, reward, next_state, over)\n",
    "      ddpg.soft_update()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "      test_result = sum([play()[-1] for _ in range(20)]) / 20\n",
    "      print(epoch, len(pool), test_result)\n",
    "      \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-6.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play(True)[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_learning2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
