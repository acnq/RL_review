{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC: discrete action, simplified\n",
    "\n",
    "soft actor-critic\n",
    "\n",
    "最大化动作的熵,增强模型的稳定性\n",
    "\n",
    "$$\n",
    "Q(s, a) + α \\text{Entropy}[Q(s, *)]\n",
    "$$\n",
    "\n",
    "训练时$α$固定，并且只用一个Value模型评估Q函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# 定义环境\n",
    "class MyWrapper(gym.Wrapper):\n",
    "  \n",
    "  def __init__(self):\n",
    "    env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "    super().__init__(env)\n",
    "    self.env = env\n",
    "    self.step_n = 0\n",
    "    \n",
    "  def reset(self):\n",
    "    state, _ = self.env.reset()\n",
    "    self.step_n = 0\n",
    "    return state\n",
    "  \n",
    "  def step(self, action):\n",
    "    state, reward, terminated, truncated, info = self.env.step(action)\n",
    "    over = terminated or truncated\n",
    "\n",
    "    #限制最大步数\n",
    "    self.step_n += 1\n",
    "    if self.step_n >= 200:\n",
    "      over = True\n",
    "\n",
    "    return state, reward, over\n",
    "  \n",
    "  # 打印游戏图像\n",
    "  def show(self):\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(self.env.render())\n",
    "    plt.show()\n",
    "    \n",
    "env = MyWrapper()\n",
    "\n",
    "env.reset()\n",
    "\n",
    "env.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5228, 0.4772],\n",
       "        [0.5143, 0.4857]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\t\n",
    "# 演员模型：计算动作概率\n",
    "model_action = torch.nn.Sequential(\n",
    "\ttorch.nn.Linear(4, 64),\n",
    "\ttorch.nn.ReLU(),\n",
    "\ttorch.nn.Linear(64, 64),\n",
    "\ttorch.nn.ReLU(),\n",
    "\ttorch.nn.Linear(64, 2),\n",
    "\ttorch.nn.Softmax(dim=1),\n",
    ")\n",
    "model_action(torch.randn(2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0789, -0.1394],\n",
       "        [-0.0782, -0.1270]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_value = torch.nn.Sequential(\n",
    "\ttorch.nn.Linear(4, 64),\n",
    "\ttorch.nn.ReLU(),\n",
    "\ttorch.nn.Linear(64, 64),\n",
    "\ttorch.nn.ReLU(),\n",
    "\ttorch.nn.Linear(64, 2),\n",
    ")\n",
    "\n",
    "model_value_next = torch.nn.Sequential(\n",
    "\ttorch.nn.Linear(4, 64),\n",
    "\ttorch.nn.ReLU(),\n",
    "\ttorch.nn.Linear(64, 64),\n",
    "\ttorch.nn.ReLU(),\n",
    "\ttorch.nn.Linear(64, 2),\n",
    ")\n",
    "\n",
    "\n",
    "model_value_next.load_state_dict(model_value.state_dict())\n",
    "model_value(torch.randn(2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "import random\n",
    "\n",
    "\n",
    "#玩一局游戏并记录数据\n",
    "def play(show=False):\n",
    "  data = []\n",
    "  reward_sum = 0\n",
    "\n",
    "  state = env.reset()\n",
    "  over = False\n",
    "  while not over:\n",
    "    # 根据环境采样\n",
    "    prob = model_action(torch.FloatTensor(state).reshape(1, 4))[0].tolist()\n",
    "    action = random.choices(range(2), weights=prob, k=1)[0]\n",
    "\n",
    "    next_state, reward, over = env.step(action)\n",
    "\n",
    "    data.append((state, action, reward, next_state, over))\n",
    "    reward_sum += reward\n",
    "    \n",
    "    state = next_state\n",
    "\n",
    "  if show:\n",
    "    display.clear_output(wait=True)\n",
    "    env.show()\n",
    "\n",
    "  return data, reward_sum\n",
    "\n",
    "play()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,\n",
       " (array([-0.04091163, -0.01071781, -0.03632788, -0.00824241], dtype=float32),\n",
       "  0,\n",
       "  1.0,\n",
       "  array([-0.04112599, -0.20530045, -0.03649273,  0.27276093], dtype=float32),\n",
       "  False))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Pool:\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.pool = []\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.pool)\n",
    "  \n",
    "  def __getitem__(self, i):\n",
    "    return self.pool[i]\n",
    "  \n",
    "  # 更新动作\n",
    "  def update(self):\n",
    "    # 每次更新至少N条数据\n",
    "    old_len = len(self.pool)\n",
    "    while len(pool) - old_len < 200:\n",
    "      self.pool.extend(play()[0])\n",
    "      \n",
    "    # 保留最新N条数据\n",
    "    self.pool = self.pool[-2_0000:]\n",
    "    \n",
    "  def sample(self):\n",
    "    data = random.sample(self.pool, 64)\n",
    "    \n",
    "    state = torch.FloatTensor([i[0] for i in data]).reshape(-1, 4)\n",
    "    action = torch.LongTensor([i[1] for i in data]).reshape(-1, 1)\n",
    "    reward = torch.FloatTensor([i[2] for i in data]).reshape(-1, 1)\n",
    "    next_state = torch.FloatTensor([i[3] for i in data]).reshape(-1, 4)\n",
    "    over = torch.LongTensor([i[4] for i in data]).reshape(-1, 1)\n",
    "    \n",
    "    return state, action, reward, next_state, over\n",
    "  \n",
    "pool = Pool()\n",
    "pool.update()\n",
    "state, action, reward, next_state, over = pool.sample()\n",
    "len(pool), pool[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_action = torch.optim.Adam(model_action.parameters(), lr=2e-4)\n",
    "optimizer_value = torch.optim.Adam(model_value.parameters(), lr=2e-3)\n",
    "\n",
    "\n",
    "def soft_update(_from, _to):\n",
    "  for _from, _to in zip(_from.parameters(), _to.parameters()):\n",
    "    value = _to.data * 0.995 + _from.data * 0.005\n",
    "    _to.data.copy_(value)\n",
    "    \n",
    "def get_prob_entroy(state):\n",
    "  prob = model_action(torch.FloatTensor(state).reshape(-1, 4))\n",
    "  entropy = prob * (prob + 1e-8).log()\n",
    "  entropy = -entropy.sum(dim=1, keepdim=True)\n",
    "  \n",
    "  return prob, entropy\n",
    "    \n",
    "def requires_grad(model, value):\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad_(value)\n",
    "    \n",
    "alpha = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0024724006652832"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# critic 模型训练\n",
    "def train_value(state, action, reward, next_state, over):\n",
    "  requires_grad(model_value, True)\n",
    "  requires_grad(model_action, False)\n",
    "  \n",
    "  # 计算targets\n",
    "  with torch.no_grad():\n",
    "    # 计算动作熵\n",
    "    prob, entropy = get_prob_entroy(next_state)\n",
    "    target = model_value_next(next_state)\n",
    "    \n",
    "  # 加权熵，越大越好\n",
    "  target = (prob * target).sum(dim=1, keepdim=True)\n",
    "  target = target + 1e-3 * entropy\n",
    "  target = target * 0.98 * (1 - over) + reward\n",
    "  \n",
    "  # 计算value\n",
    "  value = model_value(state).gather(dim=1, index=action)\n",
    "  loss = torch.nn.functional.mse_loss(value, target)\n",
    "  loss.backward()\n",
    "  optimizer_value.step()\n",
    "  optimizer_value.zero_grad()\n",
    "  \n",
    "  return loss.item()\n",
    "\n",
    "train_value(state, action, reward, next_state, over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.02478880062699318"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练actor\n",
    "def train_action(state):\n",
    "  requires_grad(model_action, True)\n",
    "  requires_grad(model_value, False)\n",
    "  \n",
    "  # 计算熵\n",
    "  prob, entropy = get_prob_entroy(state)\n",
    "  \n",
    "  # 计算value\n",
    "  value = model_value(state)\n",
    "  \n",
    "  # 求期望和\n",
    "  value = (prob * value).sum(dim=1, keepdim=True)\n",
    "  \n",
    "  # 加权熵\n",
    "  loss = -(value + 1e-3 * entropy).mean()\n",
    "  \n",
    "  loss.backward()\n",
    "  optimizer_action.step()\n",
    "  optimizer_action.zero_grad()\n",
    "  \n",
    "  return loss.item()\n",
    "\n",
    "train_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 507 1.0 21.45\n",
      "10 3221 1.0 164.1\n",
      "20 6358 1.0 182.25\n",
      "30 9475 1.0 173.7\n",
      "40 12545 1.0 175.35\n",
      "50 15406 1.0 184.6\n",
      "60 18131 1.0 198.05\n",
      "70 20000 1.0 193.0\n",
      "80 20000 1.0 190.5\n",
      "90 20000 1.0 54.7\n",
      "100 20000 1.0 200.0\n",
      "110 20000 1.0 133.15\n",
      "120 20000 1.0 102.9\n",
      "130 20000 1.0 99.6\n",
      "140 20000 1.0 96.85\n",
      "150 20000 1.0 100.7\n",
      "160 20000 1.0 107.95\n",
      "170 20000 1.0 126.2\n",
      "180 20000 1.0 170.1\n",
      "190 20000 1.0 142.65\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "def train():\n",
    "  model_action.train()\n",
    "  model_value.train()\n",
    "\n",
    "\t# 共更新N轮数据\n",
    "  for epoch in range(200):\n",
    "    # 更新N条数据\n",
    "    pool.update()\n",
    "    \n",
    "    # 每次更新过数据后,学习N次\n",
    "    for i in range(200):\n",
    "      # 采样一批\n",
    "      state, action, reward, next_state, over = pool.sample()\n",
    "\n",
    "      # 训练\n",
    "      train_value(state, action, reward, next_state, over)\n",
    "      train_action(state)\n",
    "      soft_update(model_value, model_value_next)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "      test_result = sum([play()[-1] for _ in range(20)]) / 20\n",
    "      print(epoch, len(pool), alpha, test_result)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "181.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play(True)[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_learning2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
