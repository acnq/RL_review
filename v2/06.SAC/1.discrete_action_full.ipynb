{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC: discrete action, full network\n",
    "\n",
    "soft actor-critic\n",
    "\n",
    "最大化动作的熵,增强模型的稳定性\n",
    "\n",
    "$$\n",
    "Q(s, a) + α \\text{Entropy}[Q(s, *)]\n",
    "$$\n",
    "\n",
    "训练时$α$递减，为了缓解自举，用两个Value模型评估Q函数，取小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# 定义环境\n",
    "class MyWrapper(gym.Wrapper):\n",
    "  \n",
    "  def __init__(self):\n",
    "    env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "    super().__init__(env)\n",
    "    self.env = env\n",
    "    self.step_n = 0\n",
    "    \n",
    "  def reset(self):\n",
    "    state, _ = self.env.reset()\n",
    "    self.step_n = 0\n",
    "    return state\n",
    "  \n",
    "  def step(self, action):\n",
    "    state, reward, terminated, truncated, info = self.env.step(action)\n",
    "    over = terminated or truncated\n",
    "\n",
    "    #限制最大步数\n",
    "    self.step_n += 1\n",
    "    if self.step_n >= 200:\n",
    "      over = True\n",
    "\n",
    "    return state, reward, over\n",
    "  \n",
    "  # 打印游戏图像\n",
    "  def show(self):\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(self.env.render())\n",
    "    plt.show()\n",
    "    \n",
    "env = MyWrapper()\n",
    "\n",
    "env.reset()\n",
    "\n",
    "env.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4807, 0.5193],\n",
       "        [0.5018, 0.4982]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\t\n",
    "# 演员模型：计算动作概率\n",
    "model_action = torch.nn.Sequential(\n",
    "\ttorch.nn.Linear(4, 64),\n",
    "\ttorch.nn.ReLU(),\n",
    "\ttorch.nn.Linear(64, 64),\n",
    "\ttorch.nn.ReLU(),\n",
    "\ttorch.nn.Linear(64, 2),\n",
    "\ttorch.nn.Softmax(dim=1),\n",
    ")\n",
    "model_action(torch.randn(2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0560, -0.2115],\n",
       "        [ 0.2084, -0.1923]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_value1 = torch.nn.Sequential(\n",
    "\ttorch.nn.Linear(4, 64),\n",
    "\ttorch.nn.ReLU(),\n",
    "\ttorch.nn.Linear(64, 64),\n",
    "\ttorch.nn.ReLU(),\n",
    "\ttorch.nn.Linear(64, 2),\n",
    ")\n",
    "\n",
    "model_value2 = torch.nn.Sequential(\n",
    "\ttorch.nn.Linear(4, 64),\n",
    "\ttorch.nn.ReLU(),\n",
    "\ttorch.nn.Linear(64, 64),\n",
    "\ttorch.nn.ReLU(),\n",
    "\ttorch.nn.Linear(64, 2),\n",
    ")\n",
    "\n",
    "model_value1_next = torch.nn.Sequential(\n",
    "\ttorch.nn.Linear(4, 64),\n",
    "\ttorch.nn.ReLU(),\n",
    "\ttorch.nn.Linear(64, 64),\n",
    "\ttorch.nn.ReLU(),\n",
    "\ttorch.nn.Linear(64, 2),\n",
    ")\n",
    "\n",
    "model_value2_next = torch.nn.Sequential(\n",
    "\ttorch.nn.Linear(4, 64),\n",
    "\ttorch.nn.ReLU(),\n",
    "\ttorch.nn.Linear(64, 64),\n",
    "\ttorch.nn.ReLU(),\n",
    "\ttorch.nn.Linear(64, 2),\n",
    ")\n",
    "\n",
    "model_value1_next.load_state_dict(model_value1.state_dict())\n",
    "model_value2_next.load_state_dict(model_value2.state_dict())\n",
    "model_value1(torch.randn(2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "import random\n",
    "\n",
    "\n",
    "#玩一局游戏并记录数据\n",
    "def play(show=False):\n",
    "  data = []\n",
    "  reward_sum = 0\n",
    "\n",
    "  state = env.reset()\n",
    "  over = False\n",
    "  while not over:\n",
    "    # 根据环境采样\n",
    "    prob = model_action(torch.FloatTensor(state).reshape(1, 4))[0].tolist()\n",
    "    action = random.choices(range(2), weights=prob, k=1)[0]\n",
    "\n",
    "    next_state, reward, over = env.step(action)\n",
    "\n",
    "    data.append((state, action, reward, next_state, over))\n",
    "    reward_sum += reward\n",
    "    \n",
    "    state = next_state\n",
    "\n",
    "  if show:\n",
    "    display.clear_output(wait=True)\n",
    "    env.show()\n",
    "\n",
    "  return data, reward_sum\n",
    "\n",
    "play()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215,\n",
       " (array([0.03380613, 0.02279329, 0.00323858, 0.03845455], dtype=float32),\n",
       "  1,\n",
       "  1.0,\n",
       "  array([ 0.034262  ,  0.21786866,  0.00400767, -0.25320482], dtype=float32),\n",
       "  False))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Pool:\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.pool = []\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.pool)\n",
    "  \n",
    "  def __getitem__(self, i):\n",
    "    return self.pool[i]\n",
    "  \n",
    "  # 更新动作\n",
    "  def update(self):\n",
    "    # 每次更新至少N条数据\n",
    "    old_len = len(self.pool)\n",
    "    while len(pool) - old_len < 200:\n",
    "      self.pool.extend(play()[0])\n",
    "      \n",
    "    # 保留最新N条数据\n",
    "    self.pool = self.pool[-2_0000:]\n",
    "    \n",
    "  def sample(self):\n",
    "    data = random.sample(self.pool, 64)\n",
    "    \n",
    "    state = torch.FloatTensor([i[0] for i in data]).reshape(-1, 4)\n",
    "    action = torch.LongTensor([i[1] for i in data]).reshape(-1, 1)\n",
    "    reward = torch.FloatTensor([i[2] for i in data]).reshape(-1, 1)\n",
    "    next_state = torch.FloatTensor([i[3] for i in data]).reshape(-1, 4)\n",
    "    over = torch.LongTensor([i[4] for i in data]).reshape(-1, 1)\n",
    "    \n",
    "    return state, action, reward, next_state, over\n",
    "  \n",
    "pool = Pool()\n",
    "pool.update()\n",
    "state, action, reward, next_state, over = pool.sample()\n",
    "len(pool), pool[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_action = torch.optim.Adam(model_action.parameters(), lr=2e-4)\n",
    "optimizer_value1 = torch.optim.Adam(model_value1.parameters(), lr=2e-3)\n",
    "optimizer_value2 = torch.optim.Adam(model_value2.parameters(), lr=2e-3)\n",
    "\n",
    "def soft_update(_from, _to):\n",
    "  for _from, _to in zip(_from.parameters(), _to.parameters()):\n",
    "    value = _to.data * 0.995 + _from.data * 0.005\n",
    "    _to.data.copy_(value)\n",
    "    \n",
    "def get_prob_entroy(state):\n",
    "  prob = model_action(torch.FloatTensor(state).reshape(-1, 4))\n",
    "  entropy = prob * (prob + 1e-8).log()\n",
    "  entropy = -entropy.sum(dim=1, keepdim=True)\n",
    "  \n",
    "  return prob, entropy\n",
    "    \n",
    "def requires_grad(model, value):\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad_(value)\n",
    "    \n",
    "alpha = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2396435737609863"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# critic 模型训练\n",
    "def train_value(state, action, reward, next_state, over):\n",
    "  requires_grad(model_value1, True)\n",
    "  requires_grad(model_value2, True)\n",
    "  requires_grad(model_action, False)\n",
    "  \n",
    "  # 计算targets\n",
    "  with torch.no_grad():\n",
    "    # 计算动作熵\n",
    "    prob, entropy = get_prob_entroy(next_state)\n",
    "    target1 = model_value1_next(next_state)\n",
    "    target2 = model_value1_next(next_state)\n",
    "    target = torch.min(target1, target2)\n",
    "    \n",
    "  # 加权熵，越大越好\n",
    "  target = (prob * target).sum(dim=1, keepdim=True)\n",
    "  target = target + alpha * entropy\n",
    "  target = target * 0.98 * (1 - over) + reward\n",
    "  \n",
    "  # 计算value\n",
    "  value = model_value1(state).gather(dim=1, index=action)\n",
    "  loss = torch.nn.functional.mse_loss(value, target)\n",
    "  loss.backward()\n",
    "  optimizer_value1.step()\n",
    "  optimizer_value1.zero_grad()\n",
    "  \n",
    "  value = model_value2(state).gather(dim=1, index=action)\n",
    "  loss = torch.nn.functional.mse_loss(value, target)\n",
    "  loss.backward()\n",
    "  optimizer_value2.step()\n",
    "  optimizer_value2.zero_grad()\n",
    "  \n",
    "  return loss.item()\n",
    "\n",
    "train_value(state, action, reward, next_state, over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6945646405220032"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练actor\n",
    "def train_action(state):\n",
    "  requires_grad(model_action, True)\n",
    "  requires_grad(model_value1, False)\n",
    "  requires_grad(model_value2, False)\n",
    "  \n",
    "  # 计算熵\n",
    "  prob, entropy = get_prob_entroy(state)\n",
    "  \n",
    "  # 计算value\n",
    "  value1 = model_value1(state)\n",
    "  value2 = model_value2(state)\n",
    "  value = torch.min(value1, value2)\n",
    "  \n",
    "  # 求期望和\n",
    "  value = (prob * value).sum(dim=1, keepdim=True)\n",
    "  \n",
    "  # 加权熵\n",
    "  loss = -(value + alpha * entropy).mean()\n",
    "  \n",
    "  loss.backward()\n",
    "  optimizer_action.step()\n",
    "  optimizer_action.zero_grad()\n",
    "  \n",
    "  return loss.item()\n",
    "\n",
    "train_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 416 0.9 22.85\n",
      "10 2764 0.31381059609000017 183.6\n",
      "20 5145 0.10941898913151243 191.55\n",
      "30 7504 0.03815204244769462 195.0\n",
      "40 10404 0.013302794647291147 178.55\n",
      "50 13785 0.004638397686588107 169.55\n",
      "60 16959 0.0016173092699229901 161.8\n",
      "70 19666 0.0005639208733960181 190.55\n",
      "80 20000 0.00019662705047555326 181.1\n",
      "90 20000 6.85596132412799e-05 199.75\n",
      "100 20000 2.390525899882879e-05 200.0\n",
      "110 20000 8.335248417898115e-06 200.0\n",
      "120 20000 2.9063214161987086e-06 190.95\n",
      "130 20000 1.0133716178293888e-06 198.05\n",
      "140 20000 3.5334083494636473e-07 155.1\n",
      "150 20000 1.2320233115273002e-07 200.0\n",
      "160 20000 4.295799664301754e-08 105.4\n",
      "170 20000 1.4978527259308396e-08 200.0\n",
      "180 20000 5.222689519770981e-09 200.0\n",
      "190 20000 1.821039234880364e-09 200.0\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "def train():\n",
    "  global alpha\n",
    "  model_action.train()\n",
    "  model_value1.train()\n",
    "  model_value2.train()\n",
    "\n",
    "\t# 共更新N轮数据\n",
    "  for epoch in range(200):\n",
    "    # 更新N条数据\n",
    "    pool.update()\n",
    "    \n",
    "    # 每次更新过数据后,学习N次\n",
    "    for i in range(200):\n",
    "      # 采样一批\n",
    "      state, action, reward, next_state, over = pool.sample()\n",
    "\n",
    "      # 训练\n",
    "      train_value(state, action, reward, next_state, over)\n",
    "      train_action(state)\n",
    "      soft_update(model_value1, model_value1_next)\n",
    "      soft_update(model_value2, model_value2_next)\n",
    "\n",
    "    alpha *= 0.9\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "      test_result = sum([play()[-1] for _ in range(20)]) / 20\n",
    "      print(epoch, len(pool), alpha, test_result)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play(True)[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_learning2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
